\documentclass[a4paper]{article}
\usepackage{fancyhdr}
\usepackage[includeheadfoot,left=1in, right=0.5in, top=0.5in, bottom=0.5in]{geometry}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{courier}
\usepackage{tikz}
\usepackage{color}
\usepackage{float}
\usepackage{url}
\usepackage{subfigure}
\usepackage{varwidth}
\usepackage{caption}
\usepackage{multirow}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[compact,small]{titlesec}
\usepackage{microtype}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{pdfpages}

\captionsetup[sub]{labelsep=newline}

% line spacing
\linespread{2.0}

% bold item
\let\origitem\item
\renewcommand{\item}{\normalfont\origitem}
\newcommand{\bolditem}{\small\bfseries\origitem}

% tilde
%\newcommand{\small_tilde}{\raise.17ex\hbox{$\scriptstyle\sim$}}

% indent item
\newcommand{\indentitem}{\setlength\itemindent{24pt}}

% perfect tilde
\newcommand{\tildep}{\raise.17ex\hbox{$\scriptstyle\sim$}}

\parskip = 0.5\baselineskip
\setlength{\belowcaptionskip}{-\baselineskip}

\captionsetup{font=scriptsize}
\captionsetup{labelfont=bf}

\pagestyle{fancy}
\rhead{\fancyplain{}{\rightmark }}
\lhead{\fancyplain{}{\leftmark }}
\rfoot{Page\ \thepage\ of \protect\pageref{LastPage}}
\cfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

% make verbatim text small
\makeatletter
\g@addto@macro\@verbatim\small
\makeatother

%\setlength\parindent{0pt} % Removes all indentation from paragraphs
\setlength\parindent{24pt}

\definecolor{sh_comment}{rgb}{0.12, 0.38, 0.18 } %adjusted, in Eclipse: {0.25, 0.42, 0.30 } = #3F6A4D
\definecolor{sh_keyword}{rgb}{0.37, 0.08, 0.25}  % #5F1441
\definecolor{sh_string}{rgb}{0.06, 0.10, 0.98} % #101AF9

%\sectionfont{\centering}
\lstset{
    language=vhdl,
    xleftmargin=.25in,
    xrightmargin=.25in,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    showstringspaces=false,
    captionpos=b,
    stringstyle=\color{sh_string},
    keywordstyle = \color{sh_keyword}\bfseries,
    commentstyle=\color{sh_comment}\itshape,
    basicstyle=\small\sffamily,
    %numbersep=-5pt,
    belowskip=\baselineskip,
    aboveskip=\baselineskip
}
\usepackage{authblk}

\title{
    \vspace{2in}
    \textbf{Cluster Headaches and Other Stories \\}
    \vspace{2in}
}

\author{Eric Carver}

\affil{carverer@mail.uc.edu}
    \vspace{10pt}

\affil{(513) 207-6501}
    \vspace{2.0in}

\titleformat*{\section}{\large\normalfont}

\begin{document}

%\includepdf{}
\maketitle
\thispagestyle{empty}
\newpage
\thispagestyle{empty}
\parskip = 0.2\baselineskip
\newpage
\thispagestyle{empty}
\section*{\textbf{Abstract}}
\newpage
\thispagestyle{empty}
\section*{\textbf{Acknowledgements}}
\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\listoftables
\parskip = 0.5\baselineskip
\newpage

\section{\textbf{Introduction}}



\subsection{\textbf{Research Statement}}

This research will make the 

\subsection{\textbf{Thesis Overview}}

The remainder of this thesis is organized as follows.

Chapter 2 provides background information on Parallel Discrete Event Simulation
(PDES), Infiniband\texttrademark, and small form factor computers, including the
ODROID platform used in this research.

Chapter 3 presents our strategy for attacking the network latency problem in our
Beowulf cluster. It focuses on two low-cost solutions: software RDMA over
Converged Ethernet (RoCE) and polling network drivers.

Chapter 4 describes the hardware and software implementation of the ODROID-XU
cluster constructed as part of these experiments and details the benchmarks and
simulations used to test its performance.

Chapter 5 provides the results of these tests and attempts to relate the
cluster's performance to its cost. The results of a similar set of tests on a
traditional cluster are provided for comparison.

Chapter 6 contextualizes the results of this research and suggests ways to
continue this work.

\newpage
\section{\textbf{Background}}

\subsection{\textbf{Parallel Discrete Event Simulation}}

\subsection{\textbf{Infiniband and RoCE}}

\href{www.infinibandta.org}{InfiniBand}\texttrademark (IB) was introduced in
1999 to meet the new demand of data-intensive applications in high-end computing
environments \cite{InfiniBandTABase-07}. Its characteristic high bandwidth and
low latency make it a popular interconnect technology for computing solutions
intended for fine-grained parallel simulation. For high-end Beowulf Clusters,
message latency as low as one microsecond is provided through the two-pronged
approach of using InfiniBand networking hardware and the lightweight IB
transport layer. However, the high cost and lack of inter-fabric compatibility
of such hardware have driven the development of solutions based on the
ubiquitous Ethernet link layer \cite{roce-announce}.

Although IB is a full first-order interconnect solution, its transport layer is
particularly attractive to researchers who desire affordable high performance
computing. It specifies four transport types: Reliable Connection (RC), Reliable
Datagram (RD), Unreliable Connection (UC), and Unreliable Datagram (UD)
\cite{InfiniBandTABase-07}. All four types support channel messaging semantics,
wherein messages are passed using send and receive calls
\cite{InfiniBandTABase-07}. The RC and UC types support memory semantics, also
known as Remote Direct Memory Access (RDMA). This approach allows an initiator
computing node to access the memory of a remote process directly, without any
significant effort on the part of the remote node \cite{sur-11}.

RDMA over Converged Ethernet (RoCE), also known as Infiniband over Ethernet
(IBoE), is an attempt to provide the reliable, low-latency InfiniBand transport
services over a converged Ethernet fabric already present in most data centers
\cite{InfiniBandTARoCE-10}, \cite{roce-announce}. This enables properly
configured networks to carry IB traffic without investing in entirely new
physical hardware. Message latency can be under 2 microseconds when used in a
lossless 40 Gigabit Ethernet network \cite{vienne-12}. RoCE has found a niche in
commercial data centers, especially those that support financial operations such
as high frequency trading. Unfortunately, specialized network adaptors are still
required to take advantage of its features. Additionally, these solutions
generally require switches that support Data Center Bridging (DCB)
\cite{InfiniBandTARoCE-10}. This creates an opportunity for a solution that can
work with existing lossy Ethernet networks.

%% TODO: Discuss iWARP

System Fabric Works, Inc., has created a pure software implementation of RoCE
called \href{http://www.systemfabricworks.com/downloads/roce}{rxe}. rxe is a
Linux driver that implements the full IB transport over any Ethernet adaptor
\cite{pearson-10}. Message latency can be as low as 10
microseconds\cite{pearson-10}. The
\href{http://support.systemfabricworks.com/downloads/rxe/}{most recent relase of
the driver} is in the from of patches for the mainline Linux kernel. We chose
to port this implementation to the ODROID platform for our studies in low-cost
Beowulf Clusters.


\subsection{\textbf{ODROID Platform}}

\subsubsection{\textbf{ODROID-U2}}

\subsubsection{\textbf{ODROID-XU}}

\subsection{\textbf{Related Work}}

%% ERIC: do we know of any related work?  even publications talking about
%% IBoE/RoCE performance on the x86 platform would be worth reviewing.

Although performance profiling of hardware IB and RoCE is extensive
\cite{subamaroni-09}, \cite{vienne-12}, studies on software RoCE are
rare. Robert J. Lancaster \cite{lancaster-10} experimented with the rxe
driver. He found that rxe message latency ranged from 14\% to 81\% lower than
TCP/IP using a Realtek 8168B Gigabit Ethernet adaptor with the r8169 driver.

%% Can't really find anything else on softroce

\newpage
\section{\textbf{Methods for Reducing Network Latency}}

\subsection{\textbf{Software RoCE}}

\subsubsection{\textbf{rxe}}

\subsubsection{\textbf{rxe-net}}

\subsubsection{\textbf{Porting to the ODROID Platform}}

%% TODO: Expand on this section a bunch. Include details of porting work

Although rxe is designed to operate with any Ethernet adaptor, it also depends
on the IB libraries in the Linux kernel. These libraries, in turn, depend on the
presence of a PCI bus in the host system. Neither the ODROID-U2 nor the
ODROID-XU has PCI support. Simply enabling PCI support in the Linux kernel
configuration creates some incompatibilites that must be resolved before the
kernel will work correctly. After some trial and error, I found an elegant
solution: the addition of the line in \textbf{Figure \ref{pci_code}} near the
beginning of \verb;drivers/pci/probe.c;.

\begin{figure}[h]
\begin{verbatim}
#define pcibios_assign_all_busses() 1
\end{verbatim}
\caption{Code added to drivers/pci/probe.c}
\label{pci_code}
\end{figure}

The most significant porting effort involved updating the rxe driver to work
correctly with the ARM linux memory model. ARM Linux has several subtle
differences from x86 Linux in the way the kernel maps memory. Chiefly, the rxe
code does not support the mapping of high memory (userspace memory) into low
memory (kernel memory) at runtime. This is an important feature in modern IB
drivers because it allows memory regions to be directly accessed by userspace
programs, eliminating the need for high-latency context switches. rxe currently
uses the \verb;page_address(); macro to get virtual page addresses. These calls
should be replaced by \verb;kmap(); or \verb;kmap\_atomic(); in order to make
them safe for use on high memory pages. However, each \verb;kmap(); call
requires a corresponding kunmap(). Some of the rxe DMA memory mapping functions
use the \verb;page_address(); macro to return an address for DMA
operations. This is not an intended use for \verb;kmap();, because the kernel
has a limited number of such mappings available. We were able to run the driver
recoded in this manner for use with micro-benchmarks, but it is unstable when
used with MPI applications. We concluded that a major overhaul of the rxe DMA
architecture would be required to support high memory on the driver side.

In theory, this problem is easily resolved by configuring the kernel to map all
memory to a single region, eliminating the distinction between user and kernel
address space. The ODROID-U2 only has 2047 MB of main memory, so a 32-bit
processor can map it all into a single address space. However, a peculiarity in
the way Linux interacts with the Cortex-A9 processor's memory management unit
permits only the mapping of 1747 MB of total memory using this paradigm. The
remaining 300 MB are occupied by mappings for memory-mapped I/O devices that
would normally be mapped into high memory.

%% TODO: Pending testing
%% Moving to the ODROID-XU platform obviates this problem. The Cortex-A15
%% supports Large Physical Address Extensions (LPAE). This adds a third page table
%% level, making its memory model more similar to the x86 memory model.

\newpage
\section{\textbf{Cluster Implementation and Testing}}

\newpage
\section{\textbf{Results}}

\newpage
\section{\textbf{Conclusions}}

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
