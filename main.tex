\documentclass[a4paper]{article}
\usepackage{fancyhdr}
\usepackage[includeheadfoot,left=1in, right=0.5in, top=0.5in, bottom=0.5in]{geometry}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage[usenames,dvipsnames]{color}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{courier}
\usepackage{tikz}
\usepackage{color}
\usepackage{float}
\usepackage{url}
\usepackage{subfigure}
\usepackage{varwidth}
\usepackage{caption}
\usepackage{multirow}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[compact,small]{titlesec}
\usepackage{microtype}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{epstopdf}
\usepackage{bmpsize}

\captionsetup[sub]{labelsep=newline}

% line spacing
\linespread{2.0}

% bold item
\let\origitem\item
\renewcommand{\item}{\normalfont\origitem}
\newcommand{\bolditem}{\small\bfseries\origitem}

% tilde
%\newcommand{\small_tilde}{\raise.17ex\hbox{$\scriptstyle\sim$}}

% indent item
\newcommand{\indentitem}{\setlength\itemindent{24pt}}

% perfect tilde
\newcommand{\tildep}{\raise.17ex\hbox{$\scriptstyle\sim$}}

\parskip = 0.5\baselineskip
\setlength{\belowcaptionskip}{-\baselineskip}

\captionsetup{font=scriptsize}
\captionsetup{labelfont=bf}

\pagestyle{fancy}
\rhead{\fancyplain{}{\rightmark }}
\lhead{\fancyplain{}{\leftmark }}
\rfoot{Page\ \thepage\ of \protect\pageref{LastPage}}
\cfoot{}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

% make verbatim text small
\makeatletter
\g@addto@macro\@verbatim\small
\makeatother

%\setlength\parindent{0pt} % Removes all indentation from paragraphs
\setlength\parindent{24pt}

\definecolor{sh_comment}{rgb}{0.12, 0.38, 0.18 } %adjusted, in Eclipse: {0.25, 0.42, 0.30 } = #3F6A4D
\definecolor{sh_keyword}{rgb}{0.37, 0.08, 0.25}  % #5F1441
\definecolor{sh_string}{rgb}{0.06, 0.10, 0.98} % #101AF9

%\sectionfont{\centering}
\lstset{
  language=vhdl,
  xleftmargin=.25in,
  xrightmargin=.25in,
  numbers=left,
  numberstyle=\tiny,
  frame=tb,
  showstringspaces=false,
  captionpos=b,
  stringstyle=\color{sh_string},
  keywordstyle = \color{sh_keyword}\bfseries,
  commentstyle=\color{sh_comment}\itshape,
  basicstyle=\small\sffamily,
  %numbersep=-5pt,
  belowskip=\baselineskip,
  aboveskip=\baselineskip
}
\usepackage{authblk}

\title{
  \vspace{2in}
  \textbf{Cluster Headaches and Other Stories \\}
  \vspace{2in}
}

\author{Eric Carver}

\affil{carverer@mail.uc.edu}
\vspace{10pt}

\affil{(513) 207-6501}
\vspace{2.0in}

\titleformat*{\section}{\large\normalfont}

\begin{document}

%\includepdf{}
\maketitle
\thispagestyle{empty}
\newpage
\thispagestyle{empty}
\parskip = 0.2\baselineskip
\newpage
\thispagestyle{empty}
\section*{\textbf{Abstract}}
\newpage
\thispagestyle{empty}
\section*{\textbf{Acknowledgements}}
\newpage
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\listoftables
\parskip = 0.5\baselineskip
\newpage
        
\section{\textbf{Introduction}}
\label{introduction}

\subsection{\textbf{Research Statement}}

This research will make the

\subsection{\textbf{Thesis Overview}}

The remainder of this thesis is organized as follows.

Chapter \ref{background} provides background information on Parallel Discrete
Event Simulation (PDES), Infiniband\texttrademark, and small form factor
computers, including the ODROID platform used in this research.

Chapter \ref{latency_reduction} presents our strategy for attacking the network
latency problem in our Beowulf cluster. It focuses on two low-cost solutions:
software RDMA over Converged Ethernet (RoCE) and polling network drivers.

Chapter \ref{cluster} describes the hardware and software implementation of the
ODROID-XU cluster constructed as part of these experiments and details the
benchmarks and simulations used to test its performance.

Chapter \ref{results} provides the results of these tests and attempts to relate
the cluster's performance to its cost. The results of a similar set of tests on
a traditional cluster are provided for comparison.

Chapter \ref{conclusions} contextualizes the results of this research and
suggests ways to continue this work.

\newpage
\section{\textbf{Background}}
\label{background}

\subsection{\textbf{Parallel Discrete Event Simulation}}

\subsection{\textbf{Infiniband and RoCE}}

\href{www.infinibandta.org}{InfiniBand}\texttrademark (IB) was introduced in
1999 to meet the new demand of data-intensive applications in high-end computing
environments \cite{InfiniBandTABase-07}. Its characteristic high bandwidth and
low latency make it a popular interconnect technology for computing solutions
intended for fine-grained parallel simulation. For high-end Beowulf Clusters,
message latency as low as one microsecond is provided through the two-pronged
approach of using InfiniBand networking hardware and the lightweight IB
transport layer. However, the high cost and lack of inter-fabric compatibility
of such hardware have driven the development of solutions based on the
ubiquitous Ethernet link layer \cite{roce-announce}.

Although IB is a full first-order interconnect solution, its transport layer is
particularly attractive to researchers who desire affordable high performance
computing. It specifies four transport types: Reliable Connection (RC), Reliable
Datagram (RD), Unreliable Connection (UC), and Unreliable Datagram (UD)
\cite{InfiniBandTABase-07}. All four types support channel messaging semantics,
wherein messages are passed using send and receive calls
\cite{InfiniBandTABase-07}. The RC and UC types support memory semantics, also
known as Remote Direct Memory Access (RDMA). This approach allows an initiator
computing node to access the memory of a remote process directly, without any
significant effort on the part of the remote node \cite{sur-11}.

RDMA over Converged Ethernet (RoCE), also known as Infiniband over Ethernet
(IBoE), is an attempt to provide the reliable, low-latency InfiniBand transport
services over a converged Ethernet fabric already present in most data centers
\cite{InfiniBandTARoCE-10}, \cite{roce-announce}. This enables properly
configured networks to carry IB traffic without investing in entirely new
physical hardware. Message latency can be under 2 microseconds when used in a
lossless 40 Gigabit Ethernet network \cite{vienne-12}. RoCE has found a niche in
commercial data centers, especially those that support financial operations such
as high frequency trading. Unfortunately, specialized network adaptors are still
required to take advantage of its features. Additionally, these solutions
generally require switches that support Data Center Bridging (DCB)
\cite{InfiniBandTARoCE-10}. This creates an opportunity for a solution that can
work with existing lossy Ethernet networks.

%% TODO: Discuss iWARP

System Fabric Works, Inc., has created a pure software implementation of RoCE
called \href{http://www.systemfabricworks.com/downloads/roce}{rxe}. \verb;rxe;
is a Linux driver that implements the full IB transport over any Ethernet
adaptor \cite{pearson-10}. Message latency can be as low as 10
microseconds\cite{pearson-10}. The
\href{http://support.systemfabricworks.com/downloads/rxe/}{most recent relase of
  the driver} is in the from of patches for the mainline Linux kernel. We chose
to port this implementation to the ODROID platform for our studies in low-cost
Beowulf Clusters.


\subsection{\textbf{ODROID Platform}}

\subsubsection{\textbf{ODROID-U2}}

\subsubsection{\textbf{ODROID-XU}}

\subsection{\textbf{Related Work}}

%% ERIC: do we know of any related work?  even publications talking about
%% IBoE/RoCE performance on the x86 platform would be worth reviewing.

Although performance profiling of hardware IB and RoCE is extensive
\cite{subamaroni-09}, \cite{vienne-12}, studies on software RoCE are
rare. Robert J. Lancaster \cite{lancaster-10} experimented with the rxe
driver. He found that \verb;rxe; message latency ranged from 14\% to 81\% lower
than TCP/IP using a Realtek 8168B Gigabit Ethernet adaptor with the r8169
driver.

%% Can't really find anything else on softroce

\newpage
\section{\textbf{Methods for Reducing Network Latency}}
\label{latency_reduction}

\subsection{\textbf{Software RoCE}}

Currently, there is only one open-source software RoCE project: the \verb;rxe;
linux driver. System Fabric Works created the driver with at least two goals in
mind: to provide a low-cost gateway to Infiniband software and to function as a
testbed for IB application development \cite{pearson-10}. In both cases, its
lack of network hardware dependence distinguishes it from all other Infiniband
implementations. We hoped to leverage this technology for use in our low-cost
cluster in order to bring its performance closer to the performance of a
traditional x86 cluster.

\begin{figure}[h]
\includegraphics[width=\textwidth]{rxe_linux}
\caption{rxe driver role in a Linux networking environment \cite{pearson-10}}
\label{rxe-linux}
\end{figure}

\textbf{Figure \ref{rxe-linux}} illustrates how the \verb;rxe; driver functions
as a bridge between RDMA software and the Linux networking core.

The \verb;rxe; linux driver is divided into two modules: \verb;rxe; and rxe-net
\cite{pearson-10}. \verb;rxe; provides an implementation of the Infiniband
transport compliant with the RoCE specification annex
\cite{InfiniBandTARoCE-10}. \verb;rxe-net; interfaces with the Linux networking
stack to transmit and receive packets earmarked for \verb;rxe;
\cite{pearson-10}. The following two sections detail the design of these
modules.

The \verb;rxe; driver also interfaces with a user-mode library called
librxe. This library provides the common Infiniband\texttrademark verbs
interface so that any compliant IB application can use the \verb;rxe; device
without any modification. This library is essentially a user-mode wrapper for
the functions found in the rxe driver, so its design will not be detailed here.

\subsubsection{\textbf{rxe}}
\label{rxe}

The \verb;rxe; kernel module contains the portion of the \verb;rxe; driver that
implements layers 4 through 7 of the OSI model. It provides a software
implementation of the Infiniband\texttrademark RoCE transport. As with all IB
verbs applications, memory is preallocated in Memory Regions (MR), and coupled
pairs of send and receive queues called ``queue pairs'' (QPs) are allocated by
the application based on its needs. That is, QPs are configurable at allocation
time in terms of both queue depth and entry length
\cite{InfiniBandTARoCE-10}\cite{InfiniBandTABase-07}. QPs can be resized after
they are created, but such an operation may adversely impact performance
\cite{InfiniBandTARoCE-10}.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rxe_overview}
\caption{rxe driver overview \protect\cite{pearson-10}}
\label{rxe-overview}
\end{figure}

\textbf{Figure \ref{rxe-overview}} shows the dataflow among the major partitions
of the rxe driver. When a consumer application intends to send or receive a
message, it must submit a Work Request (WR) via an IB verbs function call. This
function places the WR into a QP allocated for that application. If the
application uses Shared Receive Queue (SRQ) semantics (as OpenMPI does), then
another small wrinkle is added for receive WRs as both the SRQ and the QP
associated with the WR must be resolved. Once the Linux kernel is notified that
a WR has been enqueued, the QP Requester and QP Responder routines work to
complete that request. When a WR has been completed, and entry is placed in a
Completion Queue (CQ) that can be accessed by the verbs application. This
Completion Queue Entry (CQE) contains information that the application needs in
order to consider the WR completed.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rxe_recv}
\caption{rxe Receive task \protect\cite{pearson-10}}
\label{rxe-recv}
\end{figure}

The Layer 4 portion of the \verb;rxe; driver's Receive routine is illustrated in
\textbf{Figure \ref{rxe-recv}}. This function receives a socket buffer (SKB)
from \verb;rxe-net; as an input. It inspects the SKB header in order to
determine whether the message is a response packet or a request packet and
places the SKB into a corresponding packet list in the receive queue.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rxe_resp}
\caption{rxe QP Responder architecture \protect\cite{pearson-10}}
\label{rxe-resp}
\end{figure}

The QP Responder's architecture is illustrated in \textbf{Figure
  \ref{rxe-resp}}. The Responder partition contains functions that respond to
incoming requests for data. Such a request can be in one of two forms: a socket
buffer (SKB) sent from \verb;rxe-net; or a WR from a receive queue. In either
case, the Responder retrieves the requested data and generates a CQE
\cite{pearson-10}. If the connection associated with the request uses Reliable
Connection (RC) semantics, then a response message is generated and placed in
the send queue \cite{InfiniBandTARoCE-10}\cite{pearson-10}. Additonally, if a
request represents an atomic operation, the responder must save some state in
order to support retries for all message semantics and unordered read/write
operations for RDMA semantics. In \textbf{Figure \ref{rxe-resp}}, this is
represented by the ``Responder Resources'' data structure.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rxe_req}
\caption{rxe QP Requester architecure \protect\cite{pearson-10}}
\label{rxe-req}
\end{figure}

The QP Requester's architecture is illustrated in \textbf{Figure
  \ref{rxe-req}}. The Requester is further divided into an Initiator task and a
Completer task. The Initiator simply creates a request message for every entry
in the send queue. The Completer is active only for communication using the
Reliable Connection (RC) channel semantics. It waits for responses from the
remote node to indicate that each WQE has been retired\cite {pearson-10}. It
then generates a CQE and releases the WQE. If a message fails and must be
retried, the Initiator can be reset to the point where the Completer is
waiting. For all other channel sematics, the WQE is retired and released
immediately by the Initiator without invoking the Completer.

\begin{figure}[H]
\includegraphics[width=\textwidth]{rxe_arbiter}
\caption{rxe Arbiter architecture \protect\cite{pearson-10}}
\label{rxe-arbiter}
\end{figure}

The Arbiter task is illustrated in \textbf{Figure \ref{rxe-arbiter}}. It
schedules actual transmission of outgoing messages. It implements a round-robin
scheduling policy among all active send queues, and it also handles flow control
in order to avoid packet dropping in Linux due to queue overrun
\cite{pearson-10}. This flow control includes a static minimum time between
packet transmissions. This parameter and others in the Arbiter can be tuned
while the module is loaded, and the Arbiter can even be disabled
altogether. In our research with the ODROID-U2, lowering or removing these
timing thresholds had no effect on end-to-end latency on a 100 Mbps Ethernet
link.

\subsubsection{\textbf{rxe-net}}

In many ways, the \verb;rxe-net; module functions as a wrapper for the
\verb;rxe; transport module described in \textbf{Section \ref{rxe}}.

\subsubsection{\textbf{Porting rxe to the ODROID Platform}}

%% TODO: Expand on this section a bunch. Include details of porting work

Although \verb;rxe; is designed to operate with any Ethernet adaptor, it also
depends on the IB libraries in the Linux kernel. These libraries, in turn,
depend on the presence of a PCI bus in the host system. Neither the ODROID-U2
nor the ODROID-XU has PCI support. Simply enabling PCI support in the Linux
kernel configuration creates some incompatibilites that must be resolved before
the kernel will work correctly. After some trial and error, I found an elegant
solution: the addition of the line in \textbf{Figure \ref{pci_code}} near the
beginning of \verb;drivers/pci/probe.c;.

\begin{figure}[h]
\begin{center}
\begin{verbatim}
#define pcibios_assign_all_busses() 1
\end{verbatim}
\end{center}
\caption{Code added to drivers/pci/probe.c}
\label{pci_code}
\end{figure}

The most significant porting effort involved updating the \verb;rxe; driver to
work correctly with the ARM linux memory model. ARM Linux has several subtle
differences from x86 Linux in the way the kernel maps memory. Chiefly, the rxe
code does not support the mapping of high memory (userspace memory) into low
memory (kernel memory) at runtime. This is an important feature in modern IB
drivers because it allows memory regions to be directly accessed by userspace
programs, eliminating the need for high-latency context switches. \verb;rxe;
currently uses the \verb;page_address(); macro to get virtual page
addresses. These calls should be replaced by \verb;kmap(); or
\verb;kmap_atomic(); in order to make them safe for use on high memory
pages. However, each \verb;kmap(); call requires a corresponding
\verb;kunmap();. Some of the \verb;rxe; DMA memory mapping functions use the
\verb;page_address(); macro to return an address for DMA operations. This is not
an intended use for \verb;kmap();, because the kernel has a limited number of
such mappings available. We were able to run the driver recoded in this manner
for use with micro-benchmarks, but it is unstable when used with MPI
applications. We concluded that a major overhaul of the \verb;rxe; DMA
architecture would be required to support high memory on the driver side.

In theory, this problem is easily resolved by configuring the kernel to map all
memory to a single region, eliminating the distinction between user and kernel
address space. The ODROID-U2 only has 2047 MB of main memory, so a 32-bit
processor can map it all into a single address space. However, a peculiarity in
the way Linux interacts with the Cortex-A9 processor's memory management unit
permits only the mapping of 1747 MB of total memory using this paradigm. The
remaining 300 MB are occupied by mappings for memory-mapped I/O devices that
would normally be mapped into high memory.

%% TODO: Pending testing
%% Moving to the ODROID-XU platform obviates this problem. The Cortex-A15
%% supports Large Physical Address Extensions (LPAE). This adds a third page table
%% level, making its memory model more similar to the x86 memory model, for
%% which the \verb;rxe; driver was originally designed.

\subsection{\textbf{Polling the Network Adaptor}}

Previous research with network drivers has shown that interrupt latency in a
multi-tasking operating system can be eliminated with a polling network driver
\cite{}. In fact, the Linux TCP/IP networking stack has moved to a hybrid of
polling and interrupt-based event processing that is intended to reduce average
network latency in high-traffic situations. In Parallel Discrete Event
Simulation on multi-core processors, it is often advantageous to give up a
processor core to a polling driver in order to significantly reduce message
latency for all packets. The big.LITTLE processor configurations in products
like the ODROID-XU make this prospect especially enticing when one considers
that polling could be handled by one of the ``little'' cores, leaving all the
``big'' cores available for event processing.

Although the ability to symetrically multi-process with both ``big'' and
``little'' cores is not yet present for Linux in the ODROID-XU, we pursued this
goal in the hope that the trade of a ``big'' core for reduced latency would
still be worthwhile. Doug Weber and Zakaria Aldeneh began this study by creating
an EHCI polling driver for use with the standard ODROID-XU network
adaptors. Weber and I later extended this work to an XHCI polling driver used
with the Gigabit Ethernet adaptors for the ODROID-XU.

\subsubsection{\textbf{EHCI Polling Driver}}

%% TODO: Get info from Doug

\subsubsection{\textbf{XHCI Polling Driver}}

%% TODO: Get info from Doug

\newpage
\section{\textbf{Cluster Implementation and Testing}}
\label{cluster}

Three primary testing platforms were used for the experiments presented in this
thesis.

\subsection{\textbf{Two ODROID-U2 Nodes}}

Preliminary work with RoCE was tested on two ODROID-U2 computers running Debian
7.0-armhf with Linux kernel 3.0.90. The ODROID-U2 nodes are connected to an
unmanaged Gigabit switch through Category 6 Ethernet cables. Unless specified
otherwise, the Linux network MTU is set to 1488 bytes and the RoCE MTU is set to
1024 bytes.

\subsection{\textbf{Two ODROID-XU Nodes}}

Doug Weber and Zakaria Aldeneh performed their initial work with EHCI polling on
two ODROID-XU nodes running the Linux kernel 3.4 connected to an unmanaged
Gigabit Ethernet switch using the nodes' 100 Megabit ethernet adaptors.
%% TODO: Get more info from Doug

\subsection{\textbf{Final Cluster}}

The success of RoCE and EHCI polling experiments motivated the purchase of two
additional ODROID-XU nodes as well as Gigabit Ethernet adaptors for all four
nodes.
%% TODO: Get two nodes from Doug
%% TODO: set up cluster

\newpage
\section{\textbf{Results}}
\label{results}

\subsection{\textbf{Micro-benchmark Results}}

Preliminary micro-benchmark results from a cluster of two ODROID-U2 nodes are
presented first. Next, the results of similar tests on the four-node ODROID-XU
cluster are shown.

\subsubsection{\textbf{Two ODROID-U2 Nodes}}

Figures \ref{npmpi-llat} and \ref{npmpi-hlat} show MPI message latency for small
messages and large messages, respectively. Latency over RoCE is 17\% to 31\%
lower than latency over TCP for messages smaller than 260 B. In fact, RoCE
latency remains flat throughout that region, while TCP performance degrades
slowly as message size increases. This trend of identical performance for all
messages from 0 to around 256 B continues throughout this section. We believe
that this is a manifestation of a performance limitation introduced by the
USB-to-Ethernet translation in the LAN9730 hardware.
%% TODO: We will discuss this further in Section \ref{discussion}.

\begin{figure}[H]
\includegraphics[width=\textwidth]{netpipe_lat_small}
\caption{NetPIPE MPI Latency Results, Small Message Sizes}
\label{npmpi-llat}
\end{figure}

The trend of performance improvement with RoCE continues until message size
rises above 8 KB, after which RoCE latency is significantly higher than TCP
latency. %% This was actually really surprising. Can't explain it...yet

\begin{figure}[H]
\includegraphics[width=\textwidth]{netpipe_lat_large}
\caption{NetPIPE MPI Latency Results, Large Message Sizes}
\label{npmpi-hlat}
\end{figure}

The MPI bandwidth results in Figures \ref{npmpi-lbw} and \ref{npmpi-hbw} mirror
the latency results. Interestingly, neither TCP nor RoCE traffic could saturate
the 100 Mbps link. %% Is this because the processor can't move data around
%% fast enough? If so, this could have implications on the discussion of GbE
%% modules for the XU. We could also blame the NIC, Linux, or even NetPIPE. I
%% might have to add a way to bypass the \verb;rxe; flow control...

\begin{figure}[H]
\includegraphics[width=\textwidth]{netpipe_bw_small}
\caption{NetPIPE MPI Bandwidth Results, Small Message Sizes}
\label{npmpi-lbw}
\end{figure}



\begin{figure}[H]
\includegraphics[width=\textwidth]{netpipe_bw_large}
\caption{NetPIPE MPI Bandwidth Results, Large Message Sizes}
\label{npmpi-hbw}
\end{figure}

In order to test transport scalability over MPI in the ODROID platform, we used
the Intel MPI Benchmarks (IMB) suite's ``MultiPingPong'' test with four, six,
and eight MPI processes. This benchmark creates pairs of processes that perform
a standard ping-pong test in parallel with the other pairs. We suspected that
the reduced CPU requirements of the IB transport would lead to slower scaling of
latency as network and I/O bus congestion increases. Figure \ref{multipingpong}
supports this conjecture. In fact, the \verb;rxe; driver appears to thrive in a
congested networking environment, while TCP/IP latency grows steadily. It
appears that the increased network activity may be hiding some of the latency
introduced by the Ethernet adaptor. %% TODO: How?

\begin{figure}[H]
\includegraphics[width=\textwidth]{pingpong_multi_zoom}
\caption{Scaling of Message Latency with Link Congestion}
\label{multipingpong}
\end{figure}

\subsubsection{\textbf{Final Cluster}}

\subsection{\textbf{Parallel Discrete Event Simulation Results}}

\href{https://github.com/carothersc/ROSS}{Rensselaer's Optimistic Simulation
  System (ROSS)} is used for all PDES experiments. It uses MPI for all
inter-process communication, including local communication \cite{carothers-02}.

\newpage
\section{\textbf{Conclusions}}
\label{conclusions}

\newpage
\bibliographystyle{abbrv} \bibliography{refs}

\end{document}
